---
title: "Practical Machine Learning Assignment"
author: "R3M79"
date: "24 de Fevereiro de 2018"
output:
  html_document: default
  pdf_document: default
---

# Synopsis

This document pertais to Cousera's Practical Machine Learning final assignment. In this project, our goal is to use data from accelerometers (of devices Nike Fuelband) to perform some predictions.

# Background

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.
These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# Data

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
The data for this project come from this source: 
http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Clean Data

```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, include = FALSE)


```

First we'll load the required libraries and data

```{r loadLibrariesData,message=FALSE, warning=FALSE,include=TRUE}

# load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(AppliedPredictiveModeling)
library(rpart)
library(elasticnet)
library(gbm)
library(forecast)

# Define Variables for simulation
set.seed(100) # set the seed value for reproducibility

# Load and Clean Data
#
# Load data
training<-read.csv("./pml-training.csv")
testing<-read.csv("./pml-testing.csv")

```

By doing a quick view of data we see that we have many NA values.
We shoud first clear data with zero covariance and no relevance for the predition.
Also, the first 6 columns don't have information relevant for the prediciont so we'll remove them.

### Data Detail and Summary
```{r dataPrep,message=FALSE, warning=FALSE,include=TRUE}

# Delete variables with multiple NA using Non Zero Variance
NZV<-nearZeroVar(training)
trainSet<-training[,-NZV]
testing<-testing[,-NZV]


# Delete variables that are mostly NA
multiNA<-sapply(trainSet, function(x) mean(is.na(x))) > 0.95
trainSet<-trainSet[,multiNA==FALSE]
testing<-testing[,multiNA==FALSE]

# First 6 columns don't possess relevant information so we can delete them
trainSet<-trainSet[,-(1:6)]
testing<-testing[,-(1:6)]


```

Now that the data is prepared, in order to perform cross validation, let's create a validation set from the training set

### Validation dataset
```{r validationSet,message=FALSE, warning=FALSE,include=TRUE}
# From the training set, create a train and validation set.
# The latter will be a validation set
inTrain<-createDataPartition(trainSet$classe,p=0.7,list=FALSE)
trainSet<-trainSet[inTrain,]
validationSet<-trainSet[-inTrain,]

# Clear memory
rm(inTrain)

```


## Model Selection

With all required datasets in place, lets proceed to the Model selection.
To choose the most appropriat model let's check the accuracy of three of the most common models: Linear Distriminatory Analysis, General Boosting Model and Random Forest. 
We know that the last two are considered to be having the best overall performance among models and We expect RF model to be the best one.

###Model LDA

```{r ldaModel,message=FALSE, warning=FALSE,include=TRUE}

#fit model
modfitLDA<-train(classe~.,method="lda",data=trainSet)

#predictor
predLDA<-predict(modfitLDA,newdata=validationSet)

#Confusion Matrix
confMatLDA<-confusionMatrix(predLDA,validationSet$classe)
confMatLDA

```


#
# Model Generalized Boosting Model  

```{r gbmModel,message=FALSE, warning=FALSE,include=TRUE}

#fit model
modfitGBM<-train(classe~.,method="gbm",data=trainSet,verbose=FALSE)

#predictor
predGBM<-predict(modfitGBM,newdata=validationSet)

#Confusion Matrix
confMatGBM<-confusionMatrix(predGBM,validationSet$classe)
confMatGBM

```

# Model Random Forest

```{r rfModel,message=FALSE, warning=FALSE,include=TRUE}

#fit model
modfitRF<-train(classe~.,method="rf",data=trainSet,tr=trainControl(method="cv"),number=3)

#predictor
predRF<-predict(modfitRF,newdata=validationSet)

#Confusion Matrix
confMatRF<-confusionMatrix(predRF,validationSet$classe)
confMatRF


```


As we can see, Random Forest presents the best accuracy.
The expected out of sample error is almost 0 .

Let's now apply our final model to the testing dataset.
```{r finalPredicion,message=FALSE, warning=FALSE,include=TRUE}

finalPrediction<-predict(modfitRF,newdata=testing)
finalPrediction

```
# Appendix
## Original Training data

```{r trainingData,message=FALSE, warning=FALSE,include=TRUE}

summary(training)
str(training)

```

## Final Training data

```{r trainingDataSet,message=FALSE, warning=FALSE,include=TRUE}

summary(trainSet)
str(trainSet)

```